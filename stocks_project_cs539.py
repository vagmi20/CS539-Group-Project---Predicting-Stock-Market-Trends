# -*- coding: utf-8 -*-
"""Stocks_project_CS539.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14U-14YnpMfj3-f0atu1hGCBXMDOew8j3
"""

!pip
install
torch
torchvision
torchaudio - -index - url
https: // download.pytorch.org / whl / cu121
!pip
install
transformers[torch]
!pip
install
evaluate
!pip
install
newsapi - python
!pip
install
langdetect
!pip
install
pymongo

"""#**Dont Run The Below**

"""

import pymongo
from langdetect import detect, LangDetectException
from newsapi import NewsApiClient


class NewsArticleManager:
    def __init__(self, api_key, mongodb_uri, db_name='stocks_news', collection_name='articles'):
        self.newsapi = NewsApiClient(api_key=api_key)
        self.client = pymongo.MongoClient(mongodb_uri)
        self.db = self.client[db_name]
        self.collection = self.db[collection_name]

    def is_english(self, text):
        try:
            return detect(text) == 'en'
        except LangDetectException:
            return False

    def fetch_and_save_articles(self, keyword, from_date, to_date):
        query = keyword
        articles = self.newsapi.get_everything(q=query, from_param=from_date, to=to_date, language='en',
                                               sort_by='popularity')
        for article in articles['articles']:
            text_to_check = article.get('content') or article.get('description')
            if "[Removed]" not in article.get('title', '') and text_to_check and self.is_english(text_to_check):
                if not self.collection.find_one({'url': article['url']}):
                    self.collection.insert_one(article)

    def cleanup_articles(self):
        articles = self.collection.find()
        for article in articles:
            if "[Removed]" in article.get('title', '') or (
                    not self.is_english(article.get('content') or article.get('description', ''))):
                self.collection.delete_one({'_id': article['_id']})

    def cleanup_duplicate_articles(self):
        pipeline = [
            {"$group": {"_id": "$url", "uniqueIds": {"$addToSet": "$_id"}, "count": {"$sum": 1}}},
            {"$match": {"count": {"$gt": 1}}}
        ]
        duplicates = self.collection.aggregate(pipeline)
        for duplicate in duplicates:
            ids_to_remove = duplicate['uniqueIds'][1:]
            for id_to_remove in ids_to_remove:
                self.collection.delete_one({"_id": id_to_remove})

    def print_articles_from_mongodb(self):
        articles = self.collection.find()
        for article in articles:
            print(f"Title: {article['title']}")
            print(f"Description: {article.get('description', 'No description available')}")
            print(f"URL: {article['url']}\n")
            print("--------------------------------------------------\n")


if __name__ == '__main__':
    api_key = 'cbaf7f1f50ab40f6915bcb91db00ae1c'
    mongodb_uri = 'mongodb+srv://bsolimanhanna:K123456789@newsapitwoweeks.y1s7pil.mongodb.net/?retryWrites=true&w=majority&appName=NewsAPITwoWeeks'
    manager = NewsArticleManager(api_key, mongodb_uri)

    keywords = ["AAPL", "MSFT", "GOOGL", "AMZN", "META", "IBM", "NVDA",
                "BIDU", "CRM", "TSLA", "TWLO", "PLTR", "AI", "INTC",
                "QCOM", "AMD", "ORCL", "SAP", "SIEGY", "HON",
                "GE", "MU", "ROBO", "PATH", "ZM", "DOCU", "SQ", "SHOP",
                "SPLK", "TTD", "CRWD", "ZS", "SNOW", "FTNT", "ADSK",
                "ADBE", "ASML", "SNPS", "CDNS", "ANSS", "TER", "KYCCF",
                "OMRNY", "0020.HK", "002230.SZ", "Apple",
                "Microsoft Corporation", "Alphabet", "Amazon", "Meta", "International Business Machines Corporation",
                "NVIDIA", "Baidu", "Salesforce", "Tesla", "Twilio", "Palantir Technologies", "C3.ai", "Intel",
                "Qualcomm", "Advanced Micro Devices", "Oracle", "SAP", "Siemens", "Honeywell International",
                "General Electric", "Micron Technology",
                "Exchange Traded Concepts Trust - ROBO Global Robotics and Automation Index ETF",
                "UiPath", "Zoom Video Communications", "DocuSign", "Block", "Shopify", "Splunk", "The Trade Desk",
                "CrowdStrike Holdings", "Zscaler", "Snowflake", "Fortinet", "Autodesk", "Adobe", "ASML Holding",
                "Synopsys", "Cadence Design Systems", "ANSYS", "Teradyne", "Keyence", "Omron", "Wheelock and Company",
                "iFlytek"]
    for keyword in keywords:
        manager.fetch_and_save_articles(keyword, '2024-03-03', '2024-04-02')

    print("Finished fetching and saving articles. Now cleaning up...")
    manager.cleanup_articles()
    manager.cleanup_duplicate_articles()

    print("Now printing articles from MongoDB:\n")
    manager.print_articles_from_mongodb()

"""#**MongoDB**"""

! pip
install
pymongo
dnspython

!pip
install
"pymongo[srv]"

import pymongo
import pandas as pd

# Connects to the MongoDB server running on
# localhost:27017 by default
client = pymongo.MongoClient(
    "mongodb+srv://bsolimanhanna:K123456789@newsapitwoweeks.y1s7pil.mongodb.net/?retryWrites=true&w=majority&appName=NewsAPITwoWeeks")

db = client['stocks_news']
collection = db['articles']
all_articles = collection.find()
df = pd.DataFrame(list(all_articles))

print(df)

df.drop('_id', axis=1, inplace=True)
csv_file_path = "/content/gdrive/MyDrive/NewsAPI.csv"
df.to_csv(csv_file_path, index=False)
print(f"Data Saved to {csv_file_path}")

db = client['stocks_news']
collection = db['articles']

all_articles = collection.find()

type(all_articles)

import pandas as pd

Df = pd.DataFrame(list(all_articles))

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch


def sentiment_analysis(text, tokinizer, model):
    inputs = tokinizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=-1)
    return probs


def display_sentiment_score(collection):
    print("Sentiment Scores:")
    for article in all_articles:
        print(f"Article ID: {article['_id']} - Sentiment Score: {article.get('sentiment_score', 'Not available')}")


tokenizer = AutoTokenizer.from_pretrained("mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis")
model = AutoModelForSequenceClassification.from_pretrained(
    "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis")

scores = []
for article in all_articles:
    # article = all_articles[0]
    text = article['content']
    # text = "this is good news, stocks are soaring!"
    sentiment_probs = sentiment_analysis(text, tokenizer, model)
    # print(sentiment_probs[:])
    sentiment_score = sentiment_probs.argmax(
        dim=-1).item()  # Assuming that index 1 corresponds to the positive sentiment class
    sentiment_score_mapping = {0: "negative", 1: "neutral", 2: "positive"}
    collection.update_one({'_id': article['_id']},
                          {'$set': {'sentiment_score': sentiment_score_mapping[sentiment_score]}})
    scores.append(sentiment_score)

print(len(scores))

from google.colab import drive

drive.mount('/content/gdrive')

df = pd.DataFrame(list(all_articles))

print(df)

df.describe()

df.drop('_id', axis=1, inplace=True)
csv_file_path = "/content/gdrive/MyDrive/NewsAPI.csv"
df.to_csv(csv_file_path, index=False)
print(f"Data Saved to {csv_file_path}")

import yfinance as yf  # Import yfinance module

# Define the list of desired stock symbols
stocks = [
    "AAPL", "MSFT", "GOOGL", "AMZN", "META", "IBM", "NVDA",
    "BIDU", "CRM", "TSLA", "TWLO", "PLTR", "AI", "INTC",
    "QCOM", "AMD", "ORCL", "SAP", "SIEGY", "HON",
    "GE", "MU", "ROBO", "PATH", "ZM", "DOCU", "SQ", "SHOP",
    "SPLK", "CRWD", "ZS", "SNOW", "FTNT", "ADSK",
    "ADBE", "ASML", "SNPS", "CDNS", "ANSS", "TER", "KYCCF",
    "OMRNY", "0020.HK", "002230.SZ"
]

# Define the time period
start_date = "2024-03-03"  # Start date for fetching data
end_date = "2024-04-02"  # End date for fetching data

# Fetch historical data
data = yf.download(stocks, start=start_date, end=end_date)  # Download stock data

# Structure the data for easier analysis
summary_df = data.stack(level=1).reset_index().rename(columns={"level_1": "Ticker"})  # Stack and reset index

# Calculate average closing prices by date and add it as a new column
average_closing_by_date = data['Close'].mean(axis=1)  # Calculate average closing price
summary_df['Avg Closing Price'] = summary_df.index.map(
    average_closing_by_date)  # Map average closing price to new column

# Define the short and long windows for moving averages
short_window = 12  # Short window for moving average
long_window = 26  # Long window for moving average

# Calculate the short and long window moving averages
summary_df['SMA_12'] = summary_df.groupby('Ticker')['Close'].transform(
    lambda x: x.rolling(window=short_window).mean())  # Calculate short window moving average
summary_df['SMA_26'] = summary_df.groupby('Ticker')['Close'].transform(
    lambda x: x.rolling(window=long_window).mean())  # Calculate long window moving average

# Calculate MACD and MACD Signal
summary_df['MACD'] = summary_df['SMA_12'] - summary_df['SMA_26']  # Calculate MACD
summary_df['MACD_Signal'] = summary_df.groupby('Ticker')['MACD'].transform(
    lambda x: x.rolling(window=9).mean())  # Calculate MACD Signal

# Calculate RSI for each stock
change = summary_df.groupby('Ticker')['Close'].transform(lambda x: x.diff())  # Calculate price change
gain = change.where(change > 0, 0)  # Separate gains
loss = -change.where(change < 0, 0)  # Separate losses
avg_gain = gain.rolling(window=14).mean()  # Calculate average gain
avg_loss = loss.rolling(window=14).mean()  # Calculate average loss
rs = avg_gain / avg_loss  # Calculate relative strength
summary_df['RSI'] = 100 - (100 / (1 + rs))  # Calculate RSI

summary_df['Avg Closing Price'] = summary_df['Date'].map(data['Close'].mean(axis=1))

# Print the data types
print(summary_df.dtypes)  # Print data types of DataFrame

# Print the summary dataframe
print(summary_df)  # Print DataFrame

# Moving Averages: Help identify trends. A rising moving average indicates an uptrend,
# while a falling moving average indicates a downtrend.

# MACD: Used to catch trends early and can also indicate the end of a trend.
# A crossover of the MACD line above the signal line is a bullish signal, while a crossover below is a bearish signal.

# RSI: Identifies overbought or oversold conditions.
# Values over 70 suggest an overbought condition (potentially overvalued),
# and values under 30 suggest an oversold condition (potentially undervalued).

min_required_data_points = max(short_window, long_window, 14)
stocks_counts = summary_df['Ticker'].value_counts()
suffiecient_data_stocks = stocks_counts[stocks_counts >= min_required_data_points].index.tolist()

filtered_df = summary_df[summary_df['Ticker'].isin(suffiecient_data_stocks)]

filtered_df.describe()

summary_df.head(200)

summary_df.describe()

summary_df.count()

summary_df.isna().sum()

csv_file_path = "/content/gdrive/MyDrive/stocks.csv"
summary_df.to_csv(csv_file_path, index=False)
print(f"Data Saved to {csv_file_path}")
